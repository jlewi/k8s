{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF on GKE\n",
    "\n",
    "This notebook shows how to train [TensorFlow Inception](https://github.com/tensorflow/models/tree/master/research/slim) on GKE using [TfJobs](https://github.com/tensorflow/k8s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook you must have the following installed\n",
    "  * gcloud\n",
    "  * kubectl\n",
    "  * helm\n",
    "  * kubernetes python client library\n",
    "  \n",
    "There is a Docker image based on Datalab suitable for running this notebook.\n",
    "\n",
    "You can start that container as follows\n",
    "\n",
    "```\n",
    "docker run --name=gke-datalab -p \"127.0.0.1:8081:8080\" \\\n",
    "    -v \"${HOME}:/content/datalab/home\" \\\n",
    "    -v /var/run/docker.sock:/var/run/docker.sock -d  -e \"PROJECT_ID=\" \\\n",
    "    gcr.io/tf-on-k8s-dogfood/gke-datalab:v20171103-73616f0\n",
    "```\n",
    "  * You need to map in docker if you want tobuild docker images inside the container.\n",
    "  * Alternatively, you can set \"use_gcb\" to true in order to build the images using Google Container Builder\n",
    "  \n",
    "Additionally the [py package](https://github.com/tensorflow/k8s/tree/master/py) must be a top level package importable as py\n",
    "  * If you cloned [tensorflow/k8s](https://github.com/tensorflow/k8s) and are running this notebook in place the path with be configured automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Turn on autoreloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import a bunch of modules and set some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Assumes we are running inside the cloned repo.\n",
    "# Try to setup the path so we can import py as a top level package\n",
    "ROOT_DIR = os.path.abspath(os.path.join(\"../..\"))\n",
    "if os.path.exists(os.path.join(ROOT_DIR, \"py\")):\n",
    "  if not ROOT_DIR in sys.path:\n",
    "    sys.path.append(ROOT_DIR)\n",
    "  \n",
    "import kubernetes\n",
    "from kubernetes import client as k8s_client\n",
    "from kubernetes import config as k8s_config\n",
    "from kubernetes.client.rest import ApiException\n",
    "from kubernetes.client.models.v1_label_selector import V1LabelSelector\n",
    "import datetime\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from pprint import pprint\n",
    "try:\n",
    "  from py import build_and_push_image\n",
    "  from py import util\n",
    "except ImportError:\n",
    "  raise ImportError(\"Please ensure the py package in https://github.com/tensorflow/k8s is a top level package\")\n",
    "import StringIO\n",
    "import subprocess\n",
    "import urllib\n",
    "import urllib2\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "TF_JOB_GROUP = \"tensorflow.org\"\n",
    "TF_JOB_VERSION = \"v1alpha1\"\n",
    "TF_JOB_PLURAL = \"tfjobs\"\n",
    "TF_JOB_KIND = \"TfJob\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the notebook for your use\n",
    "Change the constants defined below.\n",
    "  1. Change **project** to a project you have access to.\n",
    "     * GKE should be enabled for that project\n",
    "  1. Change **data_dir** and **job_dir**\n",
    "     * Use a GCS bucket that you have access to\n",
    "     * Ensure the service account on your GKE cluster can read/write to this GCS bucket\n",
    "\n",
    "* Optional change the cluster name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined from the Cloud SDK configuration. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "project=\"cloud-ml-dev\"\n",
    "zone=\"us-east1-d\"\n",
    "cluster_name=\"gke-tf-example\"\n",
    "registry = \"gcr.io/\" + project\n",
    "dataset_name = \"flowers\"\n",
    "data_dir = os.path.join(\"gs://cloud-ml-dev_jlewi/inception/data\", dataset_name)\n",
    "job_dirs = \"gs://cloud-ml-dev_jlewi/inception/jobs\"\n",
    "gke = discovery.build(\"container\", \"v1\")\n",
    "namespace = \"default\"\n",
    "\n",
    "# Whether to build containers using Google Container Builder.\n",
    "# Set to false it will build by shelling out to docker build.\n",
    "use_gcb = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GKE Cluster Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The instructions below create a **CPU** cluster\n",
    "* To create a GKE cluster with GPUs sign up for the [GKE GPU Alpha](https://goo.gl/forms/ef7eh2x00hV3hahx1)\n",
    "* To use GPUs set accelerator and accelerator_count\n",
    "* For a full list of cluster options see the [Cluster object](https://cloud.google.com/container-engine/reference/rest/v1/projects.zones.clusters#Cluster) \n",
    "  in the GKE API docs\n",
    "\n",
    "To use an existing GKE cluster call **configure_kubectl** but not **create_cluster**\n",
    "\n",
    "* The code below issues a GKE request to create the cluster by calling util.create_cluster\n",
    "  * util.create_cluster uses the GKE python client library\n",
    "* After creating the cluster we call util.configure_kubectl\n",
    "  * This configures your machine to talk to the K8s master of the newly created cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:googleapiclient.discovery:URL being requested: POST https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/clusters?alt=json\n",
      "INFO:root:Creating cluster; project=cloud-ml-dev, zone=us-east1-d, name=gke-tf-example\n",
      "ERROR:root:Exception occured creating cluster: <HttpError 409 when requesting https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/clusters?alt=json returned \"The resource \"projects/cloud-ml-dev/zones/us-east1-d/clusters/gke-tf-example\" already exists.\">, status: 409\n",
      "INFO:root:Configuring kubectl\n",
      "INFO:root:Running: gcloud --project=cloud-ml-dev container clusters --zone=us-east1-d get-credentials gke-tf-example \n",
      "cwd=None\n",
      "INFO:root:Subprocess output:\n",
      "Fetching cluster endpoint and auth data.\n",
      "kubeconfig entry generated for gke-tf-example.\n",
      "\n",
      "WARNING:google.auth._default:No project ID could be determined from the Cloud SDK configuration. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "INFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): accounts.google.com\n"
     ]
    }
   ],
   "source": [
    "reload(util)\n",
    "machine_type = \"n1-standard-8\"\n",
    "use_gpu = True\n",
    "if use_gpu:\n",
    "  accelerator = \"nvidia-tesla-k80\"\n",
    "  accelerator_count = 1\n",
    "else:\n",
    "  accelerator = None\n",
    "  accelerator_count = 0\n",
    "\n",
    "cluster_request = {\n",
    "    \"cluster\": {\n",
    "        \"name\": cluster_name,\n",
    "        \"description\": \"A GKE cluster for TF.\",\n",
    "        \"initialNodeCount\": 1,\n",
    "        \"nodeConfig\": {\n",
    "            \"machineType\": machine_type,\n",
    "            \"oauthScopes\": [\n",
    "              \"https://www.googleapis.com/auth/cloud-platform\",\n",
    "            ],\n",
    "        },\n",
    "        # TODO(jlewi): Stop pinning GKE version once 1.8 becomes the default. \n",
    "        \"initialClusterVersion\": \"1.8.1-gke.1\",\n",
    "    }\n",
    "}\n",
    "\n",
    "if bool(accelerator) != (accelerator_count > 0):\n",
    "    raise ValueError(\"If accelerator is set accelerator_count must be  > 0\")\n",
    "    \n",
    "if accelerator:\n",
    "  # TODO(jlewi): Stop enabling Alpha once GPUs make it out of Alpha\n",
    "  cluster_request[\"cluster\"][\"enableKubernetesAlpha\"] = True\n",
    "\n",
    "  cluster_request[\"cluster\"][\"nodeConfig\"][\"accelerators\"] = [\n",
    "      {\n",
    "        \"acceleratorCount\": accelerator_count,\n",
    "        \"acceleratorType\": accelerator,\n",
    "      },\n",
    "  ]\n",
    "util.create_cluster(gke, project, zone, cluster_request)\n",
    "\n",
    "util.configure_kubectl(project, zone, cluster_name)\n",
    "\n",
    "k8s_config.load_kube_config()\n",
    "\n",
    "# Create an API client object to talk to the K8s master.\n",
    "api_client = k8s_client.ApiClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Operator\n",
    "\n",
    "* We need to deploy the [TfJob](https://github.com/tensorflow/k8s) custom resource on our K8s cluster\n",
    "* TfJob is deployed using the [helm](https://github.com/kubernetes/helm) package manager so first we need to setup helm on our cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating service account for tiller.\n",
      "INFO:root:Service account tiller already exists.\n",
      "INFO:root:Role binding for service account tiller already exists.\n",
      "INFO:root:Running: helm init --service-account=tiller \n",
      "cwd=None\n",
      "INFO:root:Subprocess output:\n",
      "$HELM_HOME has been configured at /root/.helm.\n",
      "Warning: Tiller is already installed in the cluster.\n",
      "(Use --client-only to suppress this message, or --upgrade to upgrade Tiller to the current version.)\n",
      "Happy Helming!\n",
      "\n",
      "INFO:root:GPUs detected in cluster.\n",
      "INFO:root:Install GPU Drivers.\n",
      "INFO:root:GPU driver daemon set has already been installed\n",
      "INFO:root:tiller is ready\n",
      "INFO:root:GPUs are available.\n"
     ]
    }
   ],
   "source": [
    "util.setup_cluster(api_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that helm is setup we can deploy the TfJob CRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running: helm install https://storage.googleapis.com/tf-on-k8s-dogfood-releases/latest/tf-job-operator-chart-latest.tgz -n tf-job --wait --replace --set rbac.install=true,cloud=gke \n",
      "cwd=None\n",
      "INFO:root:Subprocess output:\n",
      "NAME:   tf-job\n",
      "LAST DEPLOYED: Fri Nov  3 02:00:48 2017\n",
      "NAMESPACE: default\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1/Pod(related)\n",
      "NAME                             READY  STATUS   RESTARTS  AGE\n",
      "tf-job-operator-b4598cf8c-fkbc2  1/1    Running  0         2s\n",
      "\n",
      "==> v1/ConfigMap\n",
      "NAME                    DATA  AGE\n",
      "tf-job-operator-config  1     2s\n",
      "\n",
      "==> v1/ServiceAccount\n",
      "NAME             SECRETS  AGE\n",
      "tf-job-operator  1        2s\n",
      "\n",
      "==> v1beta1/ClusterRole\n",
      "NAME             AGE\n",
      "tf-job-operator  2s\n",
      "\n",
      "==> v1beta1/ClusterRoleBinding\n",
      "NAME             AGE\n",
      "tf-job-operator  2s\n",
      "\n",
      "==> v1beta1/Deployment\n",
      "NAME             DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE\n",
      "tf-job-operator  1        1        1           1          2s\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CHART=\"https://storage.googleapis.com/tf-on-k8s-dogfood-releases/latest/tf-job-operator-chart-latest.tgz\"\n",
    "util.run([\"helm\", \"install\", CHART, \"-n\", \"tf-job\", \"--wait\", \"--replace\", \"--set\", \"rbac.install=true,cloud=gke\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a TensorFlow program on K8s we need to package our code as Docker images.\n",
    "\n",
    "The [Dockerfile](https://github.com/jlewi/k8s/blob/73616f09f335defc92f9b20225c272862e92e32b/examples/tensorflow-models/Dockerfile.template) \n",
    "for this example starts with the published Docker images for TensorFlow ands \n",
    "the code for our TensorFlow program\n",
    "  * In this example we are using the CIFAR10 example in the [TensorFlow's model zoo](https://github.com/tensorflow/models)\n",
    "  * So our Dockerfile just clones that repo\n",
    "  * Using TF's Docker images ensures we start with a reliable TF environment \n",
    "\n",
    "We need to build separate Docker images for CPU and GPU versions of TensorFlow.\n",
    "  * **modes** controls whether we build images for CPU, GPU or both \n",
    "  * Our Dockerfile is a [Jinja2](http://jinja.pocoo.org/) template, so we can easily\n",
    "    build docker images based on different TensorFlow versions\n",
    "  \n",
    "The base images controls which version of TensorFlow we will use\n",
    "  * Change the base images if you want to use a different version.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT SUBMIT\n",
    "# Hack to set images to some existing ones\n",
    "images = {\n",
    "  \"cpu\": \"gcr.io/cloud-ml-dev/tf-models-cpu:397ef28-dirty-aeb4db6\",\n",
    "  \"gpu\": \"gcr.io/cloud-ml-dev/tf-models-gpu:591ca2e-dirty-24ef9d4\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:context_dir: /tmp/tmpTfJobSampleContentxtBTT7c7\n",
      "INFO:root:Running gcloud container builds submit /tmp/tmpTfJobSampleContentxtBTT7c7 --tag=gcr.io/cloud-ml-dev/tf-models-cpu:84241c2-dirty-f9a532d --project=cloud-ml-dev\n",
      "INFO:root:Creating temporary tarball archive of 3 file(s) totalling 1.6 KiB before compression.\n",
      "INFO:root:Uploading tarball of [/tmp/tmpTfJobSampleContentxtBTT7c7] to [gs://cloud-ml-dev_cloudbuild/source/1512022271.24-8d59f068b93644ab9646fc5e25a0a48e.tgz]\n",
      "INFO:root:Created [https://cloudbuild.googleapis.com/v1/projects/cloud-ml-dev/builds/016fb416-0067-4b5c-96e1-35884b6ebe0e].\n",
      "INFO:root:Logs are available at [https://console.cloud.google.com/gcr/builds/016fb416-0067-4b5c-96e1-35884b6ebe0e?project=cloud-ml-dev].\n",
      "INFO:root:----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "INFO:root:starting build \"016fb416-0067-4b5c-96e1-35884b6ebe0e\"\n",
      "INFO:root:\n",
      "INFO:root:FETCHSOURCE\n",
      "INFO:root:Fetching storage object: gs://cloud-ml-dev_cloudbuild/source/1512022271.24-8d59f068b93644ab9646fc5e25a0a48e.tgz#1512022272509545\n",
      "INFO:root:Copying gs://cloud-ml-dev_cloudbuild/source/1512022271.24-8d59f068b93644ab9646fc5e25a0a48e.tgz#1512022272509545...\n",
      "/ [1 files][  651.0 B/  651.0 B]\n",
      "INFO:root:Operation completed over 1 objects/651.0 B.\n",
      "INFO:root:BUILD\n",
      "INFO:root:Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "INFO:root:Sending build context to Docker daemon   5.12kB\n",
      "INFO:root:Step 1/3 : FROM gcr.io/tensorflow/tensorflow:1.3.0\n",
      "INFO:root:1.3.0: Pulling from tensorflow/tensorflow\n",
      "INFO:root:95f4beaf4746: Pulling fs layer\n",
      "INFO:root:a654f24b2f04: Pulling fs layer\n",
      "INFO:root:6d60ad169b64: Pulling fs layer\n",
      "INFO:root:2549c6d5adc7: Pulling fs layer\n",
      "INFO:root:c57bab9f2a29: Pulling fs layer\n",
      "INFO:root:ac20e1750c7a: Pulling fs layer\n",
      "INFO:root:5eb5e7cac016: Pulling fs layer\n",
      "INFO:root:559f0c8c16df: Pulling fs layer\n",
      "INFO:root:a87ddd485328: Pulling fs layer\n",
      "INFO:root:e51c4834e0e9: Pulling fs layer\n",
      "INFO:root:ce3f24dd1116: Pulling fs layer\n",
      "INFO:root:32b04708f9fa: Pulling fs layer\n",
      "INFO:root:2549c6d5adc7: Waiting\n",
      "INFO:root:c57bab9f2a29: Waiting\n",
      "INFO:root:ac20e1750c7a: Waiting\n",
      "INFO:root:5eb5e7cac016: Waiting\n",
      "INFO:root:559f0c8c16df: Waiting\n",
      "INFO:root:a87ddd485328: Waiting\n",
      "INFO:root:e51c4834e0e9: Waiting\n",
      "INFO:root:ce3f24dd1116: Waiting\n",
      "INFO:root:32b04708f9fa: Waiting\n",
      "INFO:root:6d60ad169b64: Verifying Checksum\n",
      "INFO:root:6d60ad169b64: Download complete\n",
      "INFO:root:a654f24b2f04: Verifying Checksum\n",
      "INFO:root:a654f24b2f04: Download complete\n",
      "INFO:root:c57bab9f2a29: Verifying Checksum\n",
      "INFO:root:c57bab9f2a29: Download complete\n",
      "INFO:root:2549c6d5adc7: Verifying Checksum\n",
      "INFO:root:2549c6d5adc7: Download complete\n",
      "INFO:root:5eb5e7cac016: Verifying Checksum\n",
      "INFO:root:5eb5e7cac016: Download complete\n",
      "INFO:root:95f4beaf4746: Verifying Checksum\n",
      "INFO:root:95f4beaf4746: Download complete\n",
      "INFO:root:95f4beaf4746: Pull complete\n",
      "INFO:root:a654f24b2f04: Pull complete\n",
      "INFO:root:6d60ad169b64: Pull complete\n",
      "INFO:root:a87ddd485328: Verifying Checksum\n",
      "INFO:root:a87ddd485328: Download complete\n",
      "INFO:root:ac20e1750c7a: Verifying Checksum\n",
      "INFO:root:ac20e1750c7a: Download complete\n",
      "INFO:root:559f0c8c16df: Verifying Checksum\n",
      "INFO:root:559f0c8c16df: Download complete\n",
      "INFO:root:e51c4834e0e9: Verifying Checksum\n",
      "INFO:root:e51c4834e0e9: Download complete\n",
      "INFO:root:32b04708f9fa: Verifying Checksum\n",
      "INFO:root:32b04708f9fa: Download complete\n",
      "INFO:root:ce3f24dd1116: Verifying Checksum\n",
      "INFO:root:ce3f24dd1116: Download complete\n",
      "INFO:root:2549c6d5adc7: Pull complete\n",
      "INFO:root:c57bab9f2a29: Pull complete\n",
      "INFO:root:ac20e1750c7a: Pull complete\n",
      "INFO:root:5eb5e7cac016: Pull complete\n",
      "INFO:root:559f0c8c16df: Pull complete\n",
      "INFO:root:a87ddd485328: Pull complete\n",
      "INFO:root:e51c4834e0e9: Pull complete\n",
      "INFO:root:ce3f24dd1116: Pull complete\n",
      "INFO:root:32b04708f9fa: Pull complete\n",
      "INFO:root:Digest: sha256:fff921f27132bdfad55a1b810a14b4bf545341ac9cffbf8a14ee2667de3c50e2\n",
      "INFO:root:Status: Downloaded newer image for gcr.io/tensorflow/tensorflow:1.3.0\n",
      "INFO:root:---> 1bb38d61d261\n",
      "INFO:root:Step 2/3 : RUN apt-get update && apt-get install -y --no-install-recommends     ca-certificates     build-essential     git\n",
      "INFO:root:---> Running in 4a351c331e55\n",
      "INFO:root:Get:1 http://security.ubuntu.com/ubuntu xenial-security InRelease [102 kB]\n",
      "INFO:root:Get:2 http://archive.ubuntu.com/ubuntu xenial InRelease [247 kB]\n",
      "INFO:root:Get:3 http://security.ubuntu.com/ubuntu xenial-security/universe Sources [53.1 kB]\n",
      "INFO:root:Get:4 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [102 kB]\n",
      "INFO:root:Get:5 http://security.ubuntu.com/ubuntu xenial-security/main amd64 Packages [505 kB]\n",
      "INFO:root:Get:6 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [102 kB]\n",
      "INFO:root:Get:7 http://archive.ubuntu.com/ubuntu xenial/universe Sources [9802 kB]\n",
      "INFO:root:Get:8 http://security.ubuntu.com/ubuntu xenial-security/restricted amd64 Packages [12.9 kB]\n",
      "INFO:root:Get:9 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 Packages [229 kB]\n",
      "INFO:root:Get:10 http://security.ubuntu.com/ubuntu xenial-security/multiverse amd64 Packages [3479 B]\n",
      "INFO:root:Get:11 http://archive.ubuntu.com/ubuntu xenial/main amd64 Packages [1558 kB]\n",
      "INFO:root:Get:12 http://archive.ubuntu.com/ubuntu xenial/restricted amd64 Packages [14.1 kB]\n",
      "INFO:root:Get:13 http://archive.ubuntu.com/ubuntu xenial/universe amd64 Packages [9827 kB]\n",
      "INFO:root:Get:14 http://archive.ubuntu.com/ubuntu xenial/multiverse amd64 Packages [176 kB]\n",
      "INFO:root:Get:15 http://archive.ubuntu.com/ubuntu xenial-updates/universe Sources [231 kB]\n",
      "INFO:root:Get:16 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [866 kB]\n",
      "INFO:root:Get:17 http://archive.ubuntu.com/ubuntu xenial-updates/restricted amd64 Packages [13.7 kB]\n",
      "INFO:root:Get:18 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [719 kB]\n",
      "INFO:root:Get:19 http://archive.ubuntu.com/ubuntu xenial-updates/multiverse amd64 Packages [18.5 kB]\n",
      "INFO:root:Get:20 http://archive.ubuntu.com/ubuntu xenial-backports/main amd64 Packages [5174 B]\n",
      "INFO:root:Get:21 http://archive.ubuntu.com/ubuntu xenial-backports/universe amd64 Packages [7150 B]\n",
      "INFO:root:Fetched 24.6 MB in 3s (6859 kB/s)\n",
      "INFO:root:Reading package lists...\n",
      "INFO:root:Reading package lists...\n",
      "INFO:root:Building dependency tree...\n",
      "INFO:root:Reading state information...\n",
      "INFO:root:build-essential is already the newest version (12.1ubuntu2).\n",
      "INFO:root:The following additional packages will be installed:\n",
      "INFO:root:git-man liberror-perl\n",
      "INFO:root:Suggested packages:\n",
      "INFO:root:gettext-base git-daemon-run | git-daemon-sysvinit git-doc git-el git-email\n",
      "INFO:root:git-gui gitk gitweb git-arch git-cvs git-mediawiki git-svn\n",
      "INFO:root:Recommended packages:\n",
      "INFO:root:less ssh-client\n",
      "INFO:root:The following NEW packages will be installed:\n",
      "INFO:root:git git-man liberror-perl\n",
      "INFO:root:The following packages will be upgraded:\n",
      "INFO:root:ca-certificates\n",
      "INFO:root:1 upgraded, 3 newly installed, 0 to remove and 52 not upgraded.\n",
      "INFO:root:Need to get 4025 kB of archives.\n",
      "INFO:root:After this operation, 25.5 MB of additional disk space will be used.\n",
      "INFO:root:Get:1 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 ca-certificates all 20170717~16.04.1 [168 kB]\n",
      "INFO:root:Get:2 http://archive.ubuntu.com/ubuntu xenial/main amd64 liberror-perl all 0.17-1.2 [19.6 kB]\n",
      "INFO:root:Get:3 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 git-man all 1:2.7.4-0ubuntu1.3 [736 kB]\n",
      "INFO:root:Get:4 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 git amd64 1:2.7.4-0ubuntu1.3 [3102 kB]\n",
      "INFO:root:\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "INFO:root:\u001b[0mFetched 4025 kB in 1s (3612 kB/s)\n",
      "(Reading database ... 13305 files and directories currently installed.)\n",
      "INFO:root:Preparing to unpack .../ca-certificates_20170717~16.04.1_all.deb ...\n",
      "INFO:root:Unpacking ca-certificates (20170717~16.04.1) over (20160104ubuntu1) ...\n",
      "INFO:root:Selecting previously unselected package liberror-perl.\n",
      "INFO:root:Preparing to unpack .../liberror-perl_0.17-1.2_all.deb ...\n",
      "INFO:root:Unpacking liberror-perl (0.17-1.2) ...\n",
      "INFO:root:Selecting previously unselected package git-man.\n",
      "INFO:root:Preparing to unpack .../git-man_1%3a2.7.4-0ubuntu1.3_all.deb ...\n",
      "INFO:root:Unpacking git-man (1:2.7.4-0ubuntu1.3) ...\n",
      "INFO:root:Selecting previously unselected package git.\n",
      "INFO:root:Preparing to unpack .../git_1%3a2.7.4-0ubuntu1.3_amd64.deb ...\n",
      "INFO:root:Unpacking git (1:2.7.4-0ubuntu1.3) ...\n",
      "INFO:root:Setting up ca-certificates (20170717~16.04.1) ...\n",
      "INFO:root:debconf: unable to initialize frontend: Dialog\n",
      "INFO:root:debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "INFO:root:debconf: falling back to frontend: Readline\n",
      "INFO:root:Setting up liberror-perl (0.17-1.2) ...\n",
      "INFO:root:Setting up git-man (1:2.7.4-0ubuntu1.3) ...\n",
      "INFO:root:Setting up git (1:2.7.4-0ubuntu1.3) ...\n",
      "INFO:root:Processing triggers for ca-certificates (20170717~16.04.1) ...\n",
      "INFO:root:Updating certificates in /etc/ssl/certs...\n",
      "INFO:root:17 added, 42 removed; done.\n",
      "INFO:root:Running hooks in /etc/ca-certificates/update.d...\n",
      "INFO:root:done.\n",
      "INFO:root:---> 85d085b86a50\n",
      "INFO:root:Removing intermediate container 4a351c331e55\n",
      "INFO:root:Step 3/3 : RUN git clone https://github.com/jlewi/models.git /tensorflow_models &&     cd /tensorflow_models &&     git checkout generate_inception_data\n",
      "INFO:root:---> Running in 3cfde80943ba\n",
      "INFO:root:\u001b[91mCloning into '/tensorflow_models'...\n",
      "INFO:root:\u001b[0m\u001b[91mSwitched to a new branch 'generate_inception_data'\n",
      "INFO:root:\u001b[0mBranch generate_inception_data set up to track remote branch generate_inception_data from origin.\n",
      "INFO:root:---> 2c1b99589460\n",
      "INFO:root:Removing intermediate container 3cfde80943ba\n",
      "INFO:root:Successfully built 2c1b99589460\n",
      "INFO:root:Successfully tagged gcr.io/cloud-ml-dev/tf-models-cpu:84241c2-dirty-f9a532d\n",
      "INFO:root:PUSH\n",
      "INFO:root:Pushing gcr.io/cloud-ml-dev/tf-models-cpu:84241c2-dirty-f9a532d\n",
      "INFO:root:The push refers to a repository [gcr.io/cloud-ml-dev/tf-models-cpu]\n",
      "INFO:root:18604b2a122b: Preparing\n",
      "INFO:root:d68f9c65ef4e: Preparing\n",
      "INFO:root:cca7884663e6: Preparing\n",
      "INFO:root:c9c04a5fd1a3: Preparing\n",
      "INFO:root:5d4dbb0c7791: Preparing\n",
      "INFO:root:6a19be88e574: Preparing\n",
      "INFO:root:adcfc17fe4eb: Preparing\n",
      "INFO:root:8f196722f8c6: Preparing\n",
      "INFO:root:eac59d81aaf0: Preparing\n",
      "INFO:root:a09947e71dc0: Preparing\n",
      "INFO:root:9c42c2077cde: Preparing\n",
      "INFO:root:625c7a2a783b: Preparing\n",
      "INFO:root:25e0901a71b8: Preparing\n",
      "INFO:root:8aa4fcad5eeb: Preparing\n",
      "INFO:root:6a19be88e574: Waiting\n",
      "INFO:root:adcfc17fe4eb: Waiting\n",
      "INFO:root:8f196722f8c6: Waiting\n",
      "INFO:root:eac59d81aaf0: Waiting\n",
      "INFO:root:a09947e71dc0: Waiting\n",
      "INFO:root:9c42c2077cde: Waiting\n",
      "INFO:root:625c7a2a783b: Waiting\n",
      "INFO:root:25e0901a71b8: Waiting\n",
      "INFO:root:8aa4fcad5eeb: Waiting\n",
      "INFO:root:c9c04a5fd1a3: Layer already exists\n",
      "INFO:root:5d4dbb0c7791: Layer already exists\n",
      "INFO:root:cca7884663e6: Layer already exists\n",
      "INFO:root:adcfc17fe4eb: Layer already exists\n",
      "INFO:root:6a19be88e574: Layer already exists\n",
      "INFO:root:8f196722f8c6: Layer already exists\n",
      "INFO:root:9c42c2077cde: Layer already exists\n",
      "INFO:root:eac59d81aaf0: Layer already exists\n",
      "INFO:root:a09947e71dc0: Layer already exists\n",
      "INFO:root:25e0901a71b8: Layer already exists\n",
      "INFO:root:625c7a2a783b: Layer already exists\n",
      "INFO:root:8aa4fcad5eeb: Layer already exists\n",
      "INFO:root:d68f9c65ef4e: Pushed\n",
      "INFO:root:18604b2a122b: Pushed\n",
      "INFO:root:84241c2-dirty-f9a532d: digest: sha256:85fb559fed14beb2ae2fb06e32a503839d079550fb49b190f4175abba152a7db size: 3255\n",
      "INFO:root:DONE\n",
      "INFO:root:--------------------------------------------------------------------------------\n",
      "INFO:root:\n",
      "INFO:root:ID                                    CREATE_TIME                DURATION  SOURCE                                                                                  IMAGES                                                   STATUS\n",
      "INFO:root:016fb416-0067-4b5c-96e1-35884b6ebe0e  2017-11-30T06:11:13+00:00  2M2S      gs://cloud-ml-dev_cloudbuild/source/1512022271.24-8d59f068b93644ab9646fc5e25a0a48e.tgz  gcr.io/cloud-ml-dev/tf-models-cpu:84241c2-dirty-f9a532d  SUCCESS\n",
      "INFO:root:context_dir: /tmp/tmpTfJobSampleContentxtiUebI7\n",
      "INFO:root:Running gcloud container builds submit /tmp/tmpTfJobSampleContentxtiUebI7 --tag=gcr.io/cloud-ml-dev/tf-models-gpu:84241c2-dirty-f9a532d --project=cloud-ml-dev\n",
      "INFO:root:Creating temporary tarball archive of 3 file(s) totalling 1.6 KiB before compression.\n",
      "INFO:root:Uploading tarball of [/tmp/tmpTfJobSampleContentxtiUebI7] to [gs://cloud-ml-dev_cloudbuild/source/1512022398.52-ec074d49ab9b474cbc508b93985baecb.tgz]\n",
      "INFO:root:Created [https://cloudbuild.googleapis.com/v1/projects/cloud-ml-dev/builds/62b50269-c3b5-4f4f-aee7-7b84c1a69a6a].\n",
      "INFO:root:Logs are available at [https://console.cloud.google.com/gcr/builds/62b50269-c3b5-4f4f-aee7-7b84c1a69a6a?project=cloud-ml-dev].\n",
      "INFO:root:----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "INFO:root:starting build \"62b50269-c3b5-4f4f-aee7-7b84c1a69a6a\"\n",
      "INFO:root:\n",
      "INFO:root:FETCHSOURCE\n",
      "INFO:root:Fetching storage object: gs://cloud-ml-dev_cloudbuild/source/1512022398.52-ec074d49ab9b474cbc508b93985baecb.tgz#1512022399640195\n",
      "INFO:root:Copying gs://cloud-ml-dev_cloudbuild/source/1512022398.52-ec074d49ab9b474cbc508b93985baecb.tgz#1512022399640195...\n",
      "/ [1 files][  655.0 B/  655.0 B]\n",
      "INFO:root:Operation completed over 1 objects/655.0 B.\n",
      "INFO:root:BUILD\n",
      "INFO:root:Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "INFO:root:Sending build context to Docker daemon   5.12kB\n",
      "INFO:root:Step 1/3 : FROM gcr.io/tensorflow/tensorflow:1.3.0-gpu\n",
      "INFO:root:1.3.0-gpu: Pulling from tensorflow/tensorflow\n",
      "INFO:root:95f4beaf4746: Pulling fs layer\n",
      "INFO:root:a654f24b2f04: Pulling fs layer\n",
      "INFO:root:6d60ad169b64: Pulling fs layer\n",
      "INFO:root:2549c6d5adc7: Pulling fs layer\n",
      "INFO:root:c57bab9f2a29: Pulling fs layer\n",
      "INFO:root:c8048c1fac7d: Pulling fs layer\n",
      "INFO:root:82a3c38c90bd: Pulling fs layer\n",
      "INFO:root:f0a6d44eb878: Pulling fs layer\n",
      "INFO:root:a19c5133aee8: Pulling fs layer\n",
      "INFO:root:e612a9066ad5: Pulling fs layer\n",
      "INFO:root:bceb15535dae: Pulling fs layer\n",
      "INFO:root:fd0b4e3934dd: Pulling fs layer\n",
      "INFO:root:3620b06eb719: Pulling fs layer\n",
      "INFO:root:847957445da4: Pulling fs layer\n",
      "INFO:root:c9019e621f12: Pulling fs layer\n",
      "INFO:root:52f32c435751: Pulling fs layer\n",
      "INFO:root:49ea3b978f49: Pulling fs layer\n",
      "INFO:root:fa3dcaf5a578: Pulling fs layer\n",
      "INFO:root:094f0f01ee9a: Pulling fs layer\n",
      "INFO:root:2549c6d5adc7: Waiting\n",
      "INFO:root:c57bab9f2a29: Waiting\n",
      "INFO:root:c8048c1fac7d: Waiting\n",
      "INFO:root:82a3c38c90bd: Waiting\n",
      "INFO:root:f0a6d44eb878: Waiting\n",
      "INFO:root:a19c5133aee8: Waiting\n",
      "INFO:root:e612a9066ad5: Waiting\n",
      "INFO:root:bceb15535dae: Waiting\n",
      "INFO:root:fd0b4e3934dd: Waiting\n",
      "INFO:root:3620b06eb719: Waiting\n",
      "INFO:root:847957445da4: Waiting\n",
      "INFO:root:c9019e621f12: Waiting\n",
      "INFO:root:52f32c435751: Waiting\n",
      "INFO:root:49ea3b978f49: Waiting\n",
      "INFO:root:fa3dcaf5a578: Waiting\n",
      "INFO:root:094f0f01ee9a: Waiting\n",
      "INFO:root:6d60ad169b64: Verifying Checksum\n",
      "INFO:root:6d60ad169b64: Download complete\n",
      "INFO:root:a654f24b2f04: Verifying Checksum\n",
      "INFO:root:a654f24b2f04: Download complete\n",
      "INFO:root:2549c6d5adc7: Verifying Checksum\n",
      "INFO:root:2549c6d5adc7: Download complete\n",
      "INFO:root:c57bab9f2a29: Verifying Checksum\n",
      "INFO:root:c57bab9f2a29: Download complete\n",
      "INFO:root:c8048c1fac7d: Verifying Checksum\n",
      "INFO:root:c8048c1fac7d: Download complete\n",
      "INFO:root:f0a6d44eb878: Verifying Checksum\n",
      "INFO:root:f0a6d44eb878: Download complete\n",
      "INFO:root:95f4beaf4746: Verifying Checksum\n",
      "INFO:root:95f4beaf4746: Download complete\n",
      "INFO:root:a19c5133aee8: Verifying Checksum\n",
      "INFO:root:a19c5133aee8: Download complete\n",
      "INFO:root:95f4beaf4746: Pull complete\n",
      "INFO:root:a654f24b2f04: Pull complete\n",
      "INFO:root:6d60ad169b64: Pull complete\n",
      "INFO:root:bceb15535dae: Verifying Checksum\n",
      "INFO:root:bceb15535dae: Download complete\n",
      "INFO:root:2549c6d5adc7: Pull complete\n",
      "INFO:root:c57bab9f2a29: Pull complete\n",
      "INFO:root:c8048c1fac7d: Pull complete\n",
      "INFO:root:fd0b4e3934dd: Verifying Checksum\n",
      "INFO:root:fd0b4e3934dd: Download complete\n",
      "INFO:root:3620b06eb719: Verifying Checksum\n",
      "INFO:root:3620b06eb719: Download complete\n",
      "INFO:root:e612a9066ad5: Verifying Checksum\n",
      "INFO:root:e612a9066ad5: Download complete\n",
      "INFO:root:847957445da4: Verifying Checksum\n",
      "INFO:root:847957445da4: Download complete\n",
      "INFO:root:c9019e621f12: Verifying Checksum\n",
      "INFO:root:c9019e621f12: Download complete\n",
      "INFO:root:52f32c435751: Verifying Checksum\n",
      "INFO:root:52f32c435751: Download complete\n",
      "INFO:root:82a3c38c90bd: Verifying Checksum\n",
      "INFO:root:82a3c38c90bd: Download complete\n",
      "INFO:root:49ea3b978f49: Verifying Checksum\n",
      "INFO:root:49ea3b978f49: Download complete\n",
      "INFO:root:fa3dcaf5a578: Verifying Checksum\n",
      "INFO:root:fa3dcaf5a578: Download complete\n",
      "INFO:root:094f0f01ee9a: Verifying Checksum\n",
      "INFO:root:094f0f01ee9a: Download complete\n",
      "INFO:root:82a3c38c90bd: Pull complete\n",
      "INFO:root:f0a6d44eb878: Pull complete\n",
      "INFO:root:a19c5133aee8: Pull complete\n",
      "INFO:root:e612a9066ad5: Pull complete\n",
      "INFO:root:bceb15535dae: Pull complete\n",
      "INFO:root:fd0b4e3934dd: Pull complete\n",
      "INFO:root:3620b06eb719: Pull complete\n",
      "INFO:root:847957445da4: Pull complete\n",
      "INFO:root:c9019e621f12: Pull complete\n",
      "INFO:root:52f32c435751: Pull complete\n",
      "INFO:root:49ea3b978f49: Pull complete\n",
      "INFO:root:fa3dcaf5a578: Pull complete\n",
      "INFO:root:094f0f01ee9a: Pull complete\n",
      "INFO:root:Digest: sha256:7844f390a9d5ff369c7756a52ca65dae095087c736935e9310e12c7caa7c73dd\n",
      "INFO:root:Status: Downloaded newer image for gcr.io/tensorflow/tensorflow:1.3.0-gpu\n",
      "INFO:root:---> 4ca1d30fcd9b\n",
      "INFO:root:Step 2/3 : RUN apt-get update && apt-get install -y --no-install-recommends     ca-certificates     build-essential     git\n",
      "INFO:root:---> Running in ad12526a219d\n",
      "INFO:root:Ign:1 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  InRelease\n",
      "INFO:root:Get:2 http://security.ubuntu.com/ubuntu xenial-security InRelease [102 kB]\n",
      "INFO:root:Get:3 http://archive.ubuntu.com/ubuntu xenial InRelease [247 kB]\n",
      "INFO:root:Ign:4 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  InRelease\n",
      "INFO:root:Get:5 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release [564 B]\n",
      "INFO:root:Get:6 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release [564 B]\n",
      "INFO:root:Get:7 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release.gpg [801 B]\n",
      "INFO:root:Get:8 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release.gpg [801 B]\n",
      "INFO:root:Get:9 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Packages [78.7 kB]\n",
      "INFO:root:Get:10 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Packages [13.0 kB]\n",
      "INFO:root:Get:11 http://security.ubuntu.com/ubuntu xenial-security/universe Sources [53.1 kB]\n",
      "INFO:root:Get:12 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [102 kB]\n",
      "INFO:root:Get:13 http://security.ubuntu.com/ubuntu xenial-security/main amd64 Packages [505 kB]\n",
      "INFO:root:Get:14 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [102 kB]\n",
      "INFO:root:Get:15 http://archive.ubuntu.com/ubuntu xenial/universe Sources [9802 kB]\n",
      "INFO:root:Get:16 http://security.ubuntu.com/ubuntu xenial-security/restricted amd64 Packages [12.9 kB]\n",
      "INFO:root:Get:17 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 Packages [229 kB]\n",
      "INFO:root:Get:18 http://security.ubuntu.com/ubuntu xenial-security/multiverse amd64 Packages [3479 B]\n",
      "INFO:root:Get:19 http://archive.ubuntu.com/ubuntu xenial/main amd64 Packages [1558 kB]\n",
      "INFO:root:Get:20 http://archive.ubuntu.com/ubuntu xenial/restricted amd64 Packages [14.1 kB]\n",
      "INFO:root:Get:21 http://archive.ubuntu.com/ubuntu xenial/universe amd64 Packages [9827 kB]\n",
      "INFO:root:Get:22 http://archive.ubuntu.com/ubuntu xenial/multiverse amd64 Packages [176 kB]\n",
      "INFO:root:Get:23 http://archive.ubuntu.com/ubuntu xenial-updates/universe Sources [231 kB]\n",
      "INFO:root:Get:24 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [866 kB]\n",
      "INFO:root:Get:25 http://archive.ubuntu.com/ubuntu xenial-updates/restricted amd64 Packages [13.7 kB]\n",
      "INFO:root:Get:26 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [719 kB]\n",
      "INFO:root:Get:27 http://archive.ubuntu.com/ubuntu xenial-updates/multiverse amd64 Packages [18.5 kB]\n",
      "INFO:root:Get:28 http://archive.ubuntu.com/ubuntu xenial-backports/main amd64 Packages [5174 B]\n",
      "INFO:root:Get:29 http://archive.ubuntu.com/ubuntu xenial-backports/universe amd64 Packages [7150 B]\n",
      "INFO:root:Fetched 24.7 MB in 3s (7129 kB/s)\n",
      "INFO:root:Reading package lists...\n",
      "INFO:root:Reading package lists...\n",
      "INFO:root:Building dependency tree...\n",
      "INFO:root:Reading state information...\n",
      "INFO:root:build-essential is already the newest version (12.1ubuntu2).\n",
      "INFO:root:The following additional packages will be installed:\n",
      "INFO:root:git-man liberror-perl\n",
      "INFO:root:Suggested packages:\n",
      "INFO:root:gettext-base git-daemon-run | git-daemon-sysvinit git-doc git-el git-email\n",
      "INFO:root:git-gui gitk gitweb git-arch git-cvs git-mediawiki git-svn\n",
      "INFO:root:Recommended packages:\n",
      "INFO:root:less ssh-client\n",
      "INFO:root:The following NEW packages will be installed:\n",
      "INFO:root:git git-man liberror-perl\n",
      "INFO:root:The following packages will be upgraded:\n",
      "INFO:root:ca-certificates\n",
      "INFO:root:1 upgraded, 3 newly installed, 0 to remove and 52 not upgraded.\n",
      "INFO:root:Need to get 4025 kB of archives.\n",
      "INFO:root:After this operation, 25.5 MB of additional disk space will be used.\n",
      "INFO:root:Get:1 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 ca-certificates all 20170717~16.04.1 [168 kB]\n",
      "INFO:root:Get:2 http://archive.ubuntu.com/ubuntu xenial/main amd64 liberror-perl all 0.17-1.2 [19.6 kB]\n",
      "INFO:root:Get:3 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 git-man all 1:2.7.4-0ubuntu1.3 [736 kB]\n",
      "INFO:root:Get:4 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 git amd64 1:2.7.4-0ubuntu1.3 [3102 kB]\n",
      "INFO:root:\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "INFO:root:\u001b[0mFetched 4025 kB in 1s (3582 kB/s)\n",
      "(Reading database ... 14853 files and directories currently installed.)\n",
      "INFO:root:Preparing to unpack .../ca-certificates_20170717~16.04.1_all.deb ...\n",
      "INFO:root:Unpacking ca-certificates (20170717~16.04.1) over (20160104ubuntu1) ...\n",
      "INFO:root:Selecting previously unselected package liberror-perl.\n",
      "INFO:root:Preparing to unpack .../liberror-perl_0.17-1.2_all.deb ...\n",
      "INFO:root:Unpacking liberror-perl (0.17-1.2) ...\n",
      "INFO:root:Selecting previously unselected package git-man.\n",
      "INFO:root:Preparing to unpack .../git-man_1%3a2.7.4-0ubuntu1.3_all.deb ...\n",
      "INFO:root:Unpacking git-man (1:2.7.4-0ubuntu1.3) ...\n",
      "INFO:root:Selecting previously unselected package git.\n",
      "INFO:root:Preparing to unpack .../git_1%3a2.7.4-0ubuntu1.3_amd64.deb ...\n",
      "INFO:root:Unpacking git (1:2.7.4-0ubuntu1.3) ...\n",
      "INFO:root:Setting up ca-certificates (20170717~16.04.1) ...\n",
      "INFO:root:debconf: unable to initialize frontend: Dialog\n",
      "INFO:root:debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "INFO:root:debconf: falling back to frontend: Readline\n",
      "INFO:root:Setting up liberror-perl (0.17-1.2) ...\n",
      "INFO:root:Setting up git-man (1:2.7.4-0ubuntu1.3) ...\n",
      "INFO:root:Setting up git (1:2.7.4-0ubuntu1.3) ...\n",
      "INFO:root:Processing triggers for ca-certificates (20170717~16.04.1) ...\n",
      "INFO:root:Updating certificates in /etc/ssl/certs...\n",
      "INFO:root:17 added, 42 removed; done.\n",
      "INFO:root:Running hooks in /etc/ca-certificates/update.d...\n",
      "INFO:root:done.\n",
      "INFO:root:---> 203b8bfcb917\n",
      "INFO:root:Removing intermediate container ad12526a219d\n",
      "INFO:root:Step 3/3 : RUN git clone https://github.com/jlewi/models.git /tensorflow_models &&     cd /tensorflow_models &&     git checkout generate_inception_data\n",
      "INFO:root:---> Running in 44323812e741\n",
      "INFO:root:\u001b[91mCloning into '/tensorflow_models'...\n",
      "INFO:root:\u001b[0m\u001b[91mSwitched to a new branch 'generate_inception_data'\n",
      "INFO:root:\u001b[0mBranch generate_inception_data set up to track remote branch generate_inception_data from origin.\n",
      "INFO:root:---> 6ee851762c06\n",
      "INFO:root:Removing intermediate container 44323812e741\n",
      "INFO:root:Successfully built 6ee851762c06\n",
      "INFO:root:Successfully tagged gcr.io/cloud-ml-dev/tf-models-gpu:84241c2-dirty-f9a532d\n",
      "INFO:root:PUSH\n",
      "INFO:root:Pushing gcr.io/cloud-ml-dev/tf-models-gpu:84241c2-dirty-f9a532d\n",
      "INFO:root:The push refers to a repository [gcr.io/cloud-ml-dev/tf-models-gpu]\n",
      "INFO:root:2a84c6c81888: Preparing\n",
      "INFO:root:7bbb1a2586a9: Preparing\n",
      "INFO:root:d0370c4745c2: Preparing\n",
      "INFO:root:3f5724c08688: Preparing\n",
      "INFO:root:b047af883cd3: Preparing\n",
      "INFO:root:d7d47b9fad98: Preparing\n",
      "INFO:root:158542675315: Preparing\n",
      "INFO:root:0a78661b279e: Preparing\n",
      "INFO:root:b03b4e1bca4c: Preparing\n",
      "INFO:root:e30fb6933f61: Preparing\n",
      "INFO:root:c1aa6c8b1868: Preparing\n",
      "INFO:root:4a4139031328: Preparing\n",
      "INFO:root:826d415e0c33: Preparing\n",
      "INFO:root:aee2d05a4618: Preparing\n",
      "INFO:root:daf2a8643391: Preparing\n",
      "INFO:root:533c6bb6b046: Preparing\n",
      "INFO:root:a09947e71dc0: Preparing\n",
      "INFO:root:9c42c2077cde: Preparing\n",
      "INFO:root:625c7a2a783b: Preparing\n",
      "INFO:root:25e0901a71b8: Preparing\n",
      "INFO:root:8aa4fcad5eeb: Preparing\n",
      "INFO:root:d7d47b9fad98: Waiting\n",
      "INFO:root:158542675315: Waiting\n",
      "INFO:root:0a78661b279e: Waiting\n",
      "INFO:root:b03b4e1bca4c: Waiting\n",
      "INFO:root:e30fb6933f61: Waiting\n",
      "INFO:root:c1aa6c8b1868: Waiting\n",
      "INFO:root:4a4139031328: Waiting\n",
      "INFO:root:826d415e0c33: Waiting\n",
      "INFO:root:aee2d05a4618: Waiting\n",
      "INFO:root:daf2a8643391: Waiting\n",
      "INFO:root:533c6bb6b046: Waiting\n",
      "INFO:root:a09947e71dc0: Waiting\n",
      "INFO:root:9c42c2077cde: Waiting\n",
      "INFO:root:625c7a2a783b: Waiting\n",
      "INFO:root:25e0901a71b8: Waiting\n",
      "INFO:root:8aa4fcad5eeb: Waiting\n",
      "INFO:root:b047af883cd3: Layer already exists\n",
      "INFO:root:3f5724c08688: Layer already exists\n",
      "INFO:root:d0370c4745c2: Layer already exists\n",
      "INFO:root:d7d47b9fad98: Layer already exists\n",
      "INFO:root:158542675315: Layer already exists\n",
      "INFO:root:0a78661b279e: Layer already exists\n",
      "INFO:root:b03b4e1bca4c: Layer already exists\n",
      "INFO:root:c1aa6c8b1868: Layer already exists\n",
      "INFO:root:e30fb6933f61: Layer already exists\n",
      "INFO:root:4a4139031328: Layer already exists\n",
      "INFO:root:826d415e0c33: Layer already exists\n",
      "INFO:root:aee2d05a4618: Layer already exists\n",
      "INFO:root:533c6bb6b046: Layer already exists\n",
      "INFO:root:daf2a8643391: Layer already exists\n",
      "INFO:root:a09947e71dc0: Layer already exists\n",
      "INFO:root:625c7a2a783b: Layer already exists\n",
      "INFO:root:9c42c2077cde: Layer already exists\n",
      "INFO:root:25e0901a71b8: Layer already exists\n",
      "INFO:root:8aa4fcad5eeb: Layer already exists\n",
      "INFO:root:7bbb1a2586a9: Pushed\n",
      "INFO:root:2a84c6c81888: Pushed\n",
      "INFO:root:84241c2-dirty-f9a532d: digest: sha256:7d7aef70aa7ec912af902b8b48e118c4de7c401314944220b7f312c73347138c size: 4726\n",
      "INFO:root:DONE\n",
      "INFO:root:--------------------------------------------------------------------------------\n",
      "INFO:root:\n",
      "INFO:root:ID                                    CREATE_TIME                DURATION  SOURCE                                                                                  IMAGES                                                   STATUS\n",
      "INFO:root:62b50269-c3b5-4f4f-aee7-7b84c1a69a6a  2017-11-30T06:13:20+00:00  3M19S     gs://cloud-ml-dev_cloudbuild/source/1512022398.52-ec074d49ab9b474cbc508b93985baecb.tgz  gcr.io/cloud-ml-dev/tf-models-gpu:84241c2-dirty-f9a532d  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "reload(build_and_push_image)\n",
    "# Set GCB project to build with GCB as opposed to locally using Docker.\n",
    "# Building on GCB can be much faster because we don't have to pull/push the images to our\n",
    "# local machine.\n",
    "gcb_project = project\n",
    "if use_gpu:\n",
    "  modes = [\"cpu\", \"gpu\"]\n",
    "else:\n",
    "  modes = [\"cpu\"]\n",
    "\n",
    "image = os.path.join(registry, \"tf-models\")\n",
    "dockerfile = os.path.join(ROOT_DIR, \"examples\", \"tensorflow-models\", \"Dockerfile.template\")\n",
    "base_images = {\n",
    "  \"cpu\": \"gcr.io/tensorflow/tensorflow:1.3.0\",\n",
    "  \"gpu\": \"gcr.io/tensorflow/tensorflow:1.3.0-gpu\",\n",
    "}\n",
    "images = build_and_push_image.build_and_push(dockerfile, image, modes=modes, base_images=base_images,\n",
    "                                             project=gcb_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Inception Datasets\n",
    "\n",
    "We need to create the TFRecord files by running [download_and_convert_data.py](https://github.com/tensorflow/models/blob/master/research/slim/download_and_convert_data.py)\n",
    "  * We submit a K8s job to run this program\n",
    "  * You can skip this step if your data is already available in data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created job inception-data-flowers-171130-062003\n"
     ]
    }
   ],
   "source": [
    "batch_api = k8s_client.BatchV1Api(api_client)\n",
    "\n",
    "job_name = \"inception-data-\" + dataset_name + \"-\"+ datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "\n",
    "body = {}\n",
    "body['apiVersion'] = \"batch/v1\"\n",
    "body['kind'] = \"Job\"\n",
    "body['metadata'] = {}\n",
    "body['metadata']['name'] = job_name\n",
    "body['metadata']['namespace'] = namespace\n",
    "\n",
    "# Note backoffLimit requires K8s >= 1.8\n",
    "spec = \"\"\"\n",
    "backoffLimit: 4\n",
    "template:\n",
    "  spec:\n",
    "    containers:\n",
    "    - name: inception\n",
    "      image: {image}\n",
    "      workingDir: /tensorflow_models/research/slim\n",
    "      command: [\"python\",  \"download_and_convert_data.py\", \"--dataset_name={dataset_name}\", \"--dataset_dir={data_dir}\"]\n",
    "    restartPolicy: Never\n",
    "\"\"\".format(data_dir=data_dir, dataset_name=dataset_name, image=images[\"cpu\"])\n",
    "\n",
    "spec_buffer = StringIO.StringIO(spec)\n",
    "body['spec'] = yaml.load(spec_buffer)\n",
    "\n",
    "try: \n",
    "    # Create a Resource\n",
    "    api_response = batch_api.create_namespaced_job(namespace, body)\n",
    "    print(\"Created job %s\" % api_response.metadata.name)\n",
    "except ApiException as e:\n",
    "    print(\n",
    "        \"Exception when calling DefaultApi->apis_fqdn_v1_namespaces_namespace_resource_post: %s\\n\" % \n",
    "        e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wait for the job to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job completed successfully\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "  results = batch_api.read_namespaced_job(job_name, namespace)\n",
    "  if results.status.succeeded >= 1 or results.status.failed >= 3:\n",
    "    break\n",
    "  print(\"Waiting for job %s ....\" % results.metadata.name)\n",
    "  time.sleep(5)\n",
    "\n",
    "if results.status.succeeded >= 1:\n",
    "  print(\"Job completed successfully\")\n",
    "else:\n",
    "  print(\"Job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a TfJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit a TfJob, we define a TfJob spec and then create it in our cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Created job inception-flowers-171130-064345\n"
     ]
    }
   ],
   "source": [
    "crd_api = k8s_client.CustomObjectsApi(api_client)\n",
    "\n",
    "namespace = \"default\"\n",
    "job_name = \"inception-\" + dataset_name + \"-\"+ datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "job_dir = os.path.join(job_dirs, job_name)\n",
    "num_steps = 10\n",
    "body = {}\n",
    "body['apiVersion'] = TF_JOB_GROUP + \"/\" + TF_JOB_VERSION\n",
    "body['kind'] = TF_JOB_KIND\n",
    "body['metadata'] = {}\n",
    "body['metadata']['name'] = job_name\n",
    "body['metadata']['namespace'] = namespace\n",
    "\n",
    "master_image = images[\"cpu\"]\n",
    "if use_gpu:\n",
    "  master_image = images[\"gpu\"]\n",
    "spec = \"\"\"\n",
    "  replicaSpecs:\n",
    "    - replicas: 1\n",
    "      tfReplicaType: MASTER\n",
    "      template:\n",
    "        spec:\n",
    "          containers:\n",
    "            - image: {master_image}\n",
    "              name: tensorflow\n",
    "              workingDir: /tensorflow_models/research/slim\n",
    "              command:\n",
    "                - python\n",
    "                - train_image_classifier.py\n",
    "                - --dataset_name={dataset_name}\n",
    "                - --dataset_split_name=train \n",
    "                - --dataset_dir={data_dir}\n",
    "                - --model_name=inception_v3\n",
    "                - --train_dir={train_dir}\n",
    "          restartPolicy: OnFailure\n",
    "  tfImage: {cpu_image}\n",
    "  tensorBoard:\n",
    "    logDir: {train_dir}\n",
    "\"\"\".format(master_image=master_image, cpu_image=images[\"cpu\"], data_dir=data_dir, \n",
    "           dataset_name=dataset_name, train_dir=job_dir)\n",
    "\n",
    "spec_buffer = StringIO.StringIO(spec)\n",
    "body['spec'] = yaml.load(spec_buffer)\n",
    "if use_gpu:\n",
    "  body['spec']['replicaSpecs'][0][\"template\"][\"spec\"][\"containers\"][0][\"resources\"] = {\n",
    "    \"limits\": {\n",
    "      \"nvidia.com/gpu\": accelerator_count,\n",
    "    }    \n",
    "  }\n",
    "\n",
    "try: \n",
    "    # Create a Resource\n",
    "    api_response = crd_api.create_namespaced_custom_object(TF_JOB_GROUP, TF_JOB_VERSION, namespace, TF_JOB_PLURAL, body) \n",
    "    logging.info(\"Created job %s\", api_response[\"metadata\"][\"name\"])\n",
    "except ApiException as e:\n",
    "    print(\n",
    "        \"Exception when calling DefaultApi->apis_fqdn_v1_namespaces_namespace_resource_post: %s\\n\" % \n",
    "        e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring your job and waiting for it to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can monitor the job a number of ways\n",
    "  * We can poll K8s to get the status of the TfJob\n",
    "  * We can check the TensorFlow logs\n",
    "      * These are available in StackDriver\n",
    "  * We can access TensorBoard if the TfJob was configured to launch TensorBoard\n",
    "  \n",
    "Running the code below will poll K8s for the TfJob status and also print out relevant links for TensorBoard and the StackDriver logs\n",
    "\n",
    "To access TensorBoard you will need to run **kubectl proxy** to create a proxy connection to your K8s cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Job has runtime id: eg5k\n",
      "INFO:root:Tensorboard will be available at job\n",
      " http://127.0.0.1:8001/api/v1/proxy/namespaces/default/services/tensorboard-eg5k:80/\n",
      "INFO:root:master pod is master-eg5k-0-p85pk\n",
      "INFO:root:Logs will be available in stackdriver at\n",
      "https://console.cloud.google.com/logs/viewer?expandAll=false&dateRangeStart=2017-11-03T03%3A00%3A43%2B00%3A00&advancedFilter=resource.type%3D%22container%22%0Aresource.labels.namespace_id%3D%22default%22%0Aresource.labels.pod_id%3D%22master-eg5k-0-p85pk%22&interval=NO_LIMIT&project=cloud-ml-dev&logName=projects%2Fcloud-ml-dev%2Flogs%2Ftensorflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Job Succeeded\n"
     ]
    }
   ],
   "source": [
    "# Get pod logs\n",
    "v1 = k8s_client.CoreV1Api(api_client)\n",
    "\n",
    "k8s_config.load_kube_config()\n",
    "api_client = k8s_client.ApiClient()\n",
    "crd_api = k8s_client.CustomObjectsApi(api_client)\n",
    "\n",
    "master_started = False\n",
    "runtime_id = None\n",
    "while True:\n",
    "  results = crd_api.get_namespaced_custom_object(TF_JOB_GROUP, TF_JOB_VERSION, namespace, TF_JOB_PLURAL, job_name)\n",
    "\n",
    "  if not runtime_id:\n",
    "    runtime_id = results[\"spec\"][\"RuntimeId\"]\n",
    "    logging.info(\"Job has runtime id: %s\", runtime_id)\n",
    "    \n",
    "    tensorboard_url = \"http://127.0.0.1:8001/api/v1/proxy/namespaces/{namespace}/services/tensorboard-{runtime_id}:80/\".format(\n",
    "    namespace=namespace, runtime_id=runtime_id)\n",
    "    logging.info(\"Tensorboard will be available at job\\n %s\", tensorboard_url)\n",
    "\n",
    "  if not master_started:\n",
    "    # Get the master pod\n",
    "    # TODO(jlewi): V1LabelSelector doesn't seem to help\n",
    "    pods = v1.list_namespaced_pod(namespace=namespace, label_selector=\"runtime_id={0},job_type=MASTER\".format(runtime_id))\n",
    "\n",
    "    # TODO(jlewi): We should probably handle the case where more than 1 pod gets started.\n",
    "    # TODO(jlewi): Once GKE logs pod labels we can just filter by labels to get all logs for a particular task\n",
    "    # and not have to identify the actual pod.\n",
    "    if pods.items:\n",
    "      pod = pods.items[0]\n",
    "\n",
    "      logging.info(\"master pod is %s\", pod.metadata.name)\n",
    "      query={\n",
    "        'advancedFilter': 'resource.type=\"container\"\\nresource.labels.namespace_id=\"default\"\\nresource.labels.pod_id=\"{0}\"'.format(pod.metadata.name), \n",
    "        'dateRangeStart': pod.metadata.creation_timestamp.isoformat(),\n",
    "        'expandAll': 'false',\n",
    "        'interval': 'NO_LIMIT',\n",
    "        'logName': 'projects/{0}/logs/tensorflow'.format(project),\n",
    "       'project': project, \n",
    "      }\n",
    "      logging.info(\"Logs will be available in stackdriver at\\n\"\n",
    "                   \"https://console.cloud.google.com/logs/viewer?\" + urllib.urlencode(query))\n",
    "      master_started = True\n",
    "\n",
    "  if results[\"status\"][\"phase\"] == \"Done\":\n",
    "    break\n",
    "  print(\"Job status {0}\".format(results[\"status\"][\"phase\"]))\n",
    "  time.sleep(5)\n",
    "  \n",
    "logging.info(\"Job %s\", results[\"status\"][\"state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "* Delete the GKE cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function delete_cluster in module py.util:\n",
      "\n",
      "delete_cluster(gke, name, project, zone)\n",
      "    Delete the cluster.\n",
      "    \n",
      "    Args:\n",
      "      gke: Client for GKE.\n",
      "      name: Name of the cluster.\n",
      "      project: Project that owns the cluster.\n",
      "      zone: Zone where the cluster is running.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "util.delete_cluster(gke, cluster_name, project, zone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): accounts.google.com\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-9187b38024ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mapi_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk8s_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApiClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk8s_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCoreV1Api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mruntime_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"spec\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"RuntimeId\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# TODO(jlewi): V1LabelSelector doesn't seem to help\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_namespaced_pod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_selector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"runtime_id={0},job_type=MASTER\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruntime_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "from kubernetes.client.models.v1_label_selector import V1LabelSelector\n",
    "import urllib2\n",
    "# Get pod logs\n",
    "k8s_config.load_kube_config()\n",
    "api_client = k8s_client.ApiClient()\n",
    "v1 = k8s_client.CoreV1Api(api_client)\n",
    "runtime_id = results[\"spec\"][\"RuntimeId\"]\n",
    "# TODO(jlewi): V1LabelSelector doesn't seem to help\n",
    "pods = v1.list_namespaced_pod(namespace=namespace, label_selector=\"runtime_id={0},job_type=MASTER\".format(runtime_id))\n",
    "\n",
    "pod = pods.items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the Pod Logs From K8s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read pod logs directly from K8s and not depend on stackdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-27 23:37:29,807 WARNING Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', error(104, 'Connection reset by peer'))': /api/v1/namespaces/default/pods/master-hrhh-0-wrh6g/log\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', error(104, 'Connection reset by peer'))': /api/v1/namespaces/default/pods/master-hrhh-0-wrh6g/log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'gs://cloud-ml-dev_jlewi/cifar10/jobs/cifar10-171027-174224', '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_session_config': gpu_options {\n",
      "  force_gpu_compatible: true\n",
      "}\n",
      "allow_soft_placement: true\n",
      ", '_tf_random_seed': None, '_task_type': u'master', '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcb715c9b10>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_log_step_count_steps': 100}\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:269: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_1/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_2/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_3/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_4/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_5/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_6/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/avg_pool/: (?, 16, 16, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_1/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_2/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_3/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_4/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_5/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_6/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/avg_pool/: (?, 8, 8, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_1/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_2/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_3/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_4/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_5/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_6/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/global_avg_pool/: (?, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/fully_connected/: (?, 11)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2017-10-27 17:42:51.299775: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-27 17:42:51.299827: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-27 17:42:51.299834: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-27 17:42:51.299839: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-27 17:42:51.299845: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into gs://cloud-ml-dev_jlewi/cifar10/jobs/cifar10-171027-174224/model.ckpt.\n",
      "INFO:tensorflow:loss = 4.74784, step = 1\n",
      "INFO:tensorflow:loss = 4.74784, learning_rate = 0.1\n",
      "INFO:tensorflow:Saving checkpoints for 10 into gs://cloud-ml-dev_jlewi/cifar10/jobs/cifar10-171027-174224/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 11.2765.\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_1/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_2/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_3/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_4/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_5/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_6/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/avg_pool/: (?, 16, 16, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_1/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_2/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_3/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_4/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_5/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_6/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/avg_pool/: (?, 8, 8, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_1/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_2/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_3/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_4/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_5/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_6/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/global_avg_pool/: (?, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/fully_connected/: (?, 11)\n",
      "INFO:tensorflow:Starting evaluation at 2017-10-27-17:44:06\n",
      "INFO:tensorflow:Restoring parameters from gs://cloud-ml-dev_jlewi/cifar10/jobs/cifar10-171027-174224/model.ckpt-10\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-27-17:45:24\n",
      "INFO:tensorflow:Saving dict for global step 10: accuracy = 0.1, global_step = 10, loss = 2.26738e+20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ret = v1.read_namespaced_pod_log(namespace=namespace, name=pod.metadata.name)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch Logs from StackDriver Programmatically\n",
    "  * On GKE pod logs are stored in stackdriver\n",
    "  * These logs will stick around longer than pod logs\n",
    "  * Fetching from stackNote this tends to be a little slow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined from the Cloud SDK configuration. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'gs://cloud-ml-dev_jlewi/cifar10/jobs/cifar10-171027-174224', '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_session_config': gpu_options {\n",
      "force_gpu_compatible: true\n",
      "}\n",
      "allow_soft_placement: true\n",
      ", '_tf_random_seed': None, '_task_type': u'master', '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcb715c9b10>, '_tf_config': gpu_options {\n",
      "per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_log_step_count_steps': 100}\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:269: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_1/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_2/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_3/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_4/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_5/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_6/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/avg_pool/: (?, 16, 16, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_1/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_2/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_3/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_4/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_5/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_6/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/avg_pool/: (?, 8, 8, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_1/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_2/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_3/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_4/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_5/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_6/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/global_avg_pool/: (?, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/fully_connected/: (?, 11)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2017-10-27 17:42:51.299775: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-27 17:42:51.299827: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-27 17:42:51.299834: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-27 17:42:51.299839: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-27 17:42:51.299845: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into gs://cloud-ml-dev_jlewi/cifar10/jobs/cifar10-171027-174224/model.ckpt.\n",
      "INFO:tensorflow:loss = 4.74784, step = 1\n",
      "INFO:tensorflow:loss = 4.74784, learning_rate = 0.1\n",
      "INFO:tensorflow:Saving checkpoints for 10 into gs://cloud-ml-dev_jlewi/cifar10/jobs/cifar10-171027-174224/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 11.2765.\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_1/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_2/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_3/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_4/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_5/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_6/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/avg_pool/: (?, 16, 16, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_1/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_2/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_3/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_4/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_5/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_6/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/avg_pool/: (?, 8, 8, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_1/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_2/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_3/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_4/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_5/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_6/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/global_avg_pool/: (?, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/fully_connected/: (?, 11)\n",
      "INFO:tensorflow:Starting evaluation at 2017-10-27-17:44:06\n",
      "INFO:tensorflow:Restoring parameters from gs://cloud-ml-dev_jlewi/cifar10/jobs/cifar10-171027-174224/model.ckpt-10\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-27-17:45:24\n",
      "INFO:tensorflow:Saving dict for global step 10: accuracy = 0.1, global_step = 10, loss = 2.26738e+20\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import logging as gcp_logging\n",
    "pod_filter = 'resource.type=\"container\" AND resource.labels.pod_id=\"master-hrhh-0-wrh6g\"'\n",
    "client = gcp_logging.Client(project=project)\n",
    "\n",
    "for entry in client.list_entries(filter_=pod_filter):\n",
    "  print(entry.payload.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
